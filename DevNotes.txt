// This is the important notes that came along the dev tour. 

-> Convertingn UIImage to cv:MAT data type: 

http://stackoverflow.com/questions/14332687/converting-uiimage-to-cvmat

-(cv::Mat)CVMat
{

CGColorSpaceRef colorSpace = CGImageGetColorSpace(self.CGImage);
CGFloat cols = self.size.width;
CGFloat rows = self.size.height;

cv::Mat cvMat(rows, cols, CV_8UC4); // 8 bits per component, 4 channels

CGContextRef contextRef = CGBitmapContextCreate(cvMat.data,                 // Pointer to backing data
                                                cols,                      // Width of bitmap
                                                rows,                     // Height of bitmap
                                                8,                          // Bits per component
                                                cvMat.step[0],              // Bytes per row
                                                colorSpace,                 // Colorspace
                                                kCGImageAlphaNoneSkipLast |
                                                kCGBitmapByteOrderDefault); // Bitmap info flags

CGContextDrawImage(contextRef, CGRectMake(0, 0, cols, rows), self.CGImage);
CGContextRelease(contextRef);

return cvMat;
}


This will be because the UIImage is not actually portrait. All photos taken with the iPhone camera are landscape in their raw bitmap state, eg 3264 wide x 2488 high. A "portrait" photo is displayed as such by the orientation EXIF flag set in the image, which is honoured, for example, by the photo library app which swivels images according to this flag and the viewing orientation of the camera.

The flag also affects how UIImage reports it's width and height properties, transposing them from their bitmap values for images flagged as portrait.

cv::Mat doesn't bother with any of that. This means that (i) when translating to cv::Mat a portrait image will have it's size.width and size.height values transposed, and (ii) when translating back from cv::Mat you will have lost the orientation flag.

The simplest way to handle this when going from UIImage to cv::mat is to swap width and height values if the image is flagged as portrait:

if  (self.imageOrientation == UIImageOrientationLeft
      || self.imageOrientation == UIImageOrientationRight) {
         cols = self.size.height;
         rows = self.size.width;
    }
When translating back from cv::mat to UIImage, you will want to reinstate the orientation flag. Assuming your cv::Mat -> UIImage code contains this:

self = [self initWithCGImage:imageRef];
you can use this method instead, and reset the orientation as per the original.

self = [self initWithCGImage:imageRef scale:1 orientation:orientation];
---------------------
This is an effort to find out how the image could be prececced: 
1- Does OpenCV compile in Objective C
2- Does CommonServices and Bus and Serv repositories are compiling successfully in c_plus_plus
3- Now, How could a basic Image processing Filter change the UIImage content: 
---------------
    
    /*
    UIImage *image = [[UIImage alloc]init];

 image = frameFromCamera;
    cv::Mat inputMat =[self cvMatFromUIImage:image];
   
    cv::Mat greyMat ;//[self cvMatGrayFromUIImage:image];
    cv::cvtColor(inputMat, greyMat, CV_BGR2GRAY);
    image = [self UIImageFromCVMat:inputMat];
 
    // Do some OpenCV stuff with the image

    cv::Mat image =[self cvMatFromUIImage:inputImage];

    
    cv::Mat image_copy;
    cv::cvtColor(image, image_copy, CV_BGRA2BGR);
    
    // invert image
    bitwise_not(image_copy, image_copy);
    cv::cvtColor(image_copy, image, CV_BGR2BGRA);

    
    UIImage* ReturnImage = [self UIImageFromCVMat:image_copy];
    
    */
    // Load cascade classifier from the XML file
/*    NSString* cascadePath = [[NSBundle mainBundle]
                             pathForResource:@"haarcascade_frontalface_alt"
                             ofType:@"xml"];
    faceDetector.load([cascadePath UTF8String]);
 
    //Load image with face
    UIImage* image = inputImage;
    cv::Mat faceImage;
    UIImageToMat(image, faceImage);
    
    // Convert to grayscale
    cv::Mat gray;
    cvtColor(faceImage, gray, CV_BGR2GRAY);
    
    // Detect faces
    std::vector<cv::Rect> faces;
        cv::CascadeClassifier faceDetector;
    faceDetector.detectMultiScale(gray, faces, 1.1,
                                  2, 0|CV_HAAR_SCALE_IMAGE, cv::Size(30, 30));
    
    // Draw all detected faces
    for(unsigned int i = 0; i < faces.size(); i++)
    {
        const cv::Rect& face = faces[i];
        // Get top-left and bottom-right corner points
        cv::Point tl(face.x, face.y);
        cv::Point br = tl + cv::Point(face.width, face.height);
        
        // Draw rectangle around the face
        cv::Scalar magenta = cv::Scalar(255, 0, 255);
        cv::rectangle(faceImage, tl, br, magenta, 4, 8, 0);
    }
    
    // Show resulting image
    UIImage * ret_image = MatToUIImage(faceImage);

*/
    /*
    
    RetroFilter *filter;
    cv::Mat frame;
    UIImageToMat(inputImage, frame);
    
    params.frameSize = frame.size();
    RetroFilter retroFilter(params);
    
    cv::Mat finalFrame;
    retroFilter.applyToPhoto(frame, finalFrame);
    
    return MatToUIImage(finalFrame);

    */

--
How could I manipulate the data content: 
